{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare-Translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data:\n",
    "The link to the dataset has been provided in the dataset section already. You can find all the Shakespearean novels there.\n",
    "<br>\n",
    "<br>\n",
    "Each novel is represented by two text files:\n",
    "<br>\n",
    "<br>\n",
    "1)The files named \"original\" contain the novels in original Shakespearean format line by line. \n",
    "<br>\n",
    "2)The same novel with name \"modern\" contains the whole novel in modern english format. \n",
    "<br>\n",
    "<br>\n",
    "In this way, the same line in a novel can be extracted in Shakespearean format from the \"original\" file and in modern format from the \"modern\" file.\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData():\n",
    "    orgfilnames = ['hamlet_original.snt.aligned', 'julius-caesar_original.snt.aligned', 'macbeth_original.snt.aligned', 'merchant-of-venice_original.snt.aligned', 'midsummer-nights-dream_original.snt.aligned', 'othello_original.snt.aligned', 'romeo-and-juliet_original.snt.aligned', 'tempest_original.snt.aligned']\n",
    "    \n",
    "    with open('orgfile.txt', 'w') as outfile:\n",
    "        for fname in orgfilnames:\n",
    "            with open(fname, encoding='utf-8', errors = 'ignore' ) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "    original = open('orgfile.txt', encoding='utf-8', errors = 'ignore').read().split('\\n')\n",
    "    modfilnames = ['hamlet_modern.snt.aligned', 'julius-caesar_modern.snt.aligned', 'macbeth_modern.snt.aligned', 'merchant-of-venice_modern.snt.aligned', 'midsummer-nights-dream_modern.snt.aligned', 'othello_modern.snt.aligned', 'romeo-and-juliet_modern.snt.aligned', 'tempest_modern.snt.aligned']\n",
    "    with open('modfile.txt', 'w') as outfile:\n",
    "        for fname in modfilnames:\n",
    "            with open(fname, encoding='utf-8', errors = 'ignore' ) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)    \n",
    "    modern = open('modfile.txt', encoding='utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages:\n",
    "All package imports go here.\n",
    "<br>\n",
    "<br>\n",
    "The basic required packages have been given in requirements.txt which can be installed using the pip command given in the <strong>usage section</strong>.\n",
    "<br>\n",
    "<br>\n",
    "For example:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import tensorflow\n",
    "import keras\n",
    "import matplotlib\n",
    "#Import the rest of the libraries in a similar fashion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data:\n",
    "<strong>Step 1:</strong><br>\n",
    "<br>\n",
    "Since we will be dealing majorly with alphabets, convert all the text to lowercase. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowerCase(corpus):\n",
    "    {\n",
    "                \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Step 2:</strong><br>\n",
    "<br>\n",
    "Tokenize a text file into a list of sentences. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(corpus):\n",
    "    {\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Step 3:</strong><br>\n",
    "<br>\n",
    "Tokenize a sentence into a bag of words. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sentence):\n",
    "    {\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Step 4:</strong><br>\n",
    "<br>\n",
    "Convert a sentence into an equivalent list of characters without punctuation(.,?,!). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfAlphabets(sentence):\n",
    "    {\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing data:\n",
    "The text data needs to be converted to vector form before it can be used for training and testing purposes. \n",
    "<br>\n",
    "<br>\n",
    "Using numpy arrays is a great way of vectorising data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    {\n",
    "        #Hint: Convert a sentence into a numpy array. Since the length of each sentence varies, find the length of the longest sentence first(this length becomes the length of the training vectors) and zero pad the sentences smaller in length that the longest.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and testing data:\n",
    "Split the vectorized data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data):\n",
    "    {\n",
    "        #Split the data.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model:\n",
    "Now that the data has been cleaned, preprocessed and vectorized, it is ready to be put to use.\n",
    "<br>\n",
    "<br>\n",
    "The next step is to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    {\n",
    "        #Build the model.\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:\n",
    "Train you model or fit the data to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_data):\n",
    "    {\n",
    "        #Train the model.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model:\n",
    "Test out your trained model and plot results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_data):\n",
    "    {\n",
    "        #Test the model.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve Accuracy:\n",
    "Find ways to improve the model and increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve(model):\n",
    "    {\n",
    "        #Improve the model.\n",
    "    } "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
